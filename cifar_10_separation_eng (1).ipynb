{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE7oyG0wv6e0"
      },
      "source": [
        "#Separation of CIFAR-10 Images\n",
        "\n",
        "The model takes as input an image created by averaging two random samples from CIFAR-10 and is tasked with predicting the categories of the two components.\n",
        "\n",
        "The first image belongs to the first five categories (airplane, automobile, bird, cat, deer), while the second belongs to the remaining categories (dog, frog, horse, ship, truck). The model must return two labels, each within a range of five possible values.\n",
        "\n",
        "The evaluation metric for the model is as follows: calculate the classification accuracy for the two component images and then compute their average.\n",
        "\n",
        "The metric should be evaluated on 10,000 inputs generated from test data. Repeat the calculation 10 times and measure the standard deviation, which must be reported.\n",
        "\n",
        "A data generator and some examples are provided below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USdmzjiO0W6D"
      },
      "source": [
        "#Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "iHjnh5XP0Sq4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRYiW2ipukZF",
        "outputId": "25d5ed64-63ae-4d8a-9790-56e40d4e3992"
      },
      "outputs": [],
      "source": [
        "(cifar10_x_train, cifar10_y_train), (cifar10_x_test, cifar10_y_test) = cifar10.load_data()\n",
        "assert cifar10_x_train.shape == (50000, 32, 32, 3)\n",
        "assert cifar10_x_test.shape == (10000, 32, 32, 3)\n",
        "assert cifar10_y_train.shape == (50000, 1)\n",
        "assert cifar10_y_test.shape == (10000, 1)\n",
        "\n",
        "classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "\n",
        "cifar10_x_train = (cifar10_x_train/255.).astype(np.float32)\n",
        "cifar10_x_test = (cifar10_x_test/255.).astype(np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkiGnU4d0k4d"
      },
      "source": [
        "Let us split the images in two groups, according to their label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Dpey42Vo07Yb"
      },
      "outputs": [],
      "source": [
        "cond_1 = cifar10_y_train[:,0] < 5\n",
        "cifar10_x_train_1 = cifar10_x_train[cond_1]\n",
        "cifar10_y_train_1 = cifar10_y_train[cond_1]\n",
        "\n",
        "cond_2 = cifar10_y_train[:,0] >= 5\n",
        "cifar10_x_train_2 = cifar10_x_train[cond_2]\n",
        "cifar10_y_train_2 = cifar10_y_train[cond_2]\n",
        "\n",
        "cond_1_test = cifar10_y_test[:,0] < 5\n",
        "cifar10_x_test_1 = cifar10_x_test[cond_1_test]\n",
        "cifar10_y_test_1 = cifar10_y_test[cond_1_test]\n",
        "\n",
        "cond_2_test = cifar10_y_test[:,0] >= 5\n",
        "cifar10_x_test_2 = cifar10_x_test[cond_2_test]\n",
        "cifar10_y_test_2 = cifar10_y_test[cond_2_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmLYNuR-0s0m"
      },
      "source": [
        "Now we can define the generator. The input consists of two datasets (X1,X2), their corresponding labels (Y1,Y2), and a batch size.\n",
        "\n",
        "The generator returns (x_data,y_data), where:\n",
        "* x_data is a batch of images obtained by averaging random samples from X1 and X2.\n",
        "* y_data is a pair of batches of labels corresponding to the component images, expressed in categorical format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7Y5Zpv5fw2hd"
      },
      "outputs": [],
      "source": [
        "def datagenerator(X1, X2, Y1, Y2, batchsize):\n",
        "    size1 = X1.shape[0]\n",
        "    size2 = X2.shape[0]\n",
        "    Y1_cat = tf.keras.utils.to_categorical(Y1, num_classes=5)\n",
        "    Y2_cat = tf.keras.utils.to_categorical(Y2 - 5, num_classes=5)\n",
        "\n",
        "    while True:\n",
        "        num1 = np.random.randint(0, size1, batchsize)\n",
        "        num2 = np.random.randint(0, size2, batchsize)\n",
        "        x_data = (X1[num1] + X2[num2]) / 2.0\n",
        "        y_data = (Y1_cat[num1], Y2_cat[num2])\n",
        "        yield x_data, y_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9lf3TuP2pdQ"
      },
      "source": [
        "\n",
        "Let us instantiate a generator on Cifar10 with batchsize=1, and let's see its behaviour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "29TldJ6-720b"
      },
      "outputs": [],
      "source": [
        "datagen = datagenerator(cifar10_x_train_1,cifar10_x_train_2,cifar10_y_train_1,cifar10_y_train_2,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1DrJVzI3ysV"
      },
      "source": [
        "Let's generate an example, display the image that the model will take as input, and print the categories of the two overlapping components.\n",
        "\n",
        "You can re-run the cell to display new examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qL1sMtjG8VmG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "first: airplane, second = truck\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x2a8222304d0>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "x, y = next(datagen)\n",
        "\n",
        "print(\"first: {}, second = {}\".format(classes[np.argmax(y[0][0])],classes[np.argmax(y[1][0])+5]))\n",
        "#print(np.min(x[0]),np.max(x[0]))\n",
        "plt.imshow(x[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5lzBotwL5QN"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_p4UuG1QF8t"
      },
      "source": [
        "Let us define first of all the test generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "cQo8_6w-L4WY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000, 32, 32, 3)\n"
          ]
        }
      ],
      "source": [
        "testgen = datagenerator(cifar10_x_test_1,cifar10_x_test_2,cifar10_y_test_1,cifar10_y_test_2,10000)\n",
        "\n",
        "eval_samples_x, eval_samples_y = next(testgen)\n",
        "print(eval_samples_x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MiLnkKROGCD"
      },
      "source": [
        "We now test a model producing random guesses. You will need to replace it with your own predictive model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1GllTEtPN_xv"
      },
      "outputs": [],
      "source": [
        "def random_model(x):\n",
        "  #the random model ingnore the input x and return a pair of random classes\n",
        "  return(np.random.randint(0,5,(10000,2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "gomFTuuDOy8A"
      },
      "outputs": [],
      "source": [
        "def eval_model(model):\n",
        "  eval_samples_x, eval_samples_y = next(testgen)\n",
        "  random_guesses = model(eval_samples_x)\n",
        "  correct_guesses_1 = random_guesses[:,0] == np.argmax(eval_samples_y[0],axis=1)\n",
        "  correct_guesses_2 = random_guesses[:,1] == np.argmax(eval_samples_y[1],axis=1)\n",
        "  return (np.mean(correct_guesses_1) + np.mean(correct_guesses_2))/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "i4AL2M6yjJno"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.19915"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_model(random_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7usBI88dje70"
      },
      "source": [
        "As expected, the accuracy is around 1/5 = 0.2\n",
        "\n",
        "Let us repeat the evaluation ten times, and compute the standard deviation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ZFu8iEt9jdZA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean accuracy =  0.20027499999999998\n",
            "standard deviation =  0.0017261590309122752\n"
          ]
        }
      ],
      "source": [
        "repeat_eval = 10\n",
        "eval_results = []\n",
        "for i in range(repeat_eval):\n",
        "  eval_results.append(eval_model(random_model))\n",
        "print(\"mean accuracy = \", np.mean(eval_results))\n",
        "print(\"standard deviation = \", np.std(eval_results))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment 2: Unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\n# Define U-Net Model\\ndef unet_model():\\n    inputs = tf.keras.Input(shape=(32, 32, 3))\\n\\n    # Encoder\\n    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\\n    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\\n    p1 = layers.MaxPooling2D((2, 2))(c1)\\n\\n    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\\n    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\\n    p2 = layers.MaxPooling2D((2, 2))(c2)\\n\\n    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\\n    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\\n    p3 = layers.MaxPooling2D((2, 2))(c3)\\n\\n    # Bottleneck\\n    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\\n    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\\n\\n    # Decoder\\n    u1 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c4)\\n    u1 = layers.concatenate([u1, c3])\\n    c5 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u1)\\n    c5 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c5)\\n\\n    u2 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\\n    u2 = layers.concatenate([u2, c2])\\n    c6 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u2)\\n    c6 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c6)\\n\\n    u3 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\\n    u3 = layers.concatenate([u3, c1])\\n    c7 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u3)\\n    c7 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c7)\\n\\n    # Output layers for each label\\n    c7_flattened = layers.Flatten()(c7)\\n    output1 = layers.Dense(5, activation='softmax', name='output1')(c7_flattened)\\n    output2 = layers.Dense(5, activation='softmax', name='output2')(c7_flattened)\\n\\n    model = Model(inputs=inputs, outputs=[output1, output2])\\n    return model\\n\""
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "# Define U-Net Model\n",
        "def unet_model():\n",
        "    inputs = tf.keras.Input(shape=(32, 32, 3))\n",
        "\n",
        "    # Encoder\n",
        "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
        "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
        "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
        "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
        "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
        "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    # Bottleneck\n",
        "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
        "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
        "\n",
        "    # Decoder\n",
        "    u1 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c4)\n",
        "    u1 = layers.concatenate([u1, c3])\n",
        "    c5 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u1)\n",
        "    c5 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c5)\n",
        "\n",
        "    u2 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u2 = layers.concatenate([u2, c2])\n",
        "    c6 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u2)\n",
        "    c6 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c6)\n",
        "\n",
        "    u3 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u3 = layers.concatenate([u3, c1])\n",
        "    c7 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u3)\n",
        "    c7 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c7)\n",
        "\n",
        "    # Output layers for each label\n",
        "    c7_flattened = layers.Flatten()(c7)\n",
        "    output1 = layers.Dense(5, activation='softmax', name='output1')(c7_flattened)\n",
        "    output2 = layers.Dense(5, activation='softmax', name='output2')(c7_flattened)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=[output1, output2])\n",
        "    return model\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_deep_unet(input_shape):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
        "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
        "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
        "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
        "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
        "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
        "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
        "    p4 = layers.MaxPooling2D((2, 2))(c4)\n",
        "\n",
        "    c5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
        "    c5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
        "    p5 = layers.MaxPooling2D((2, 2))(c5)\n",
        "\n",
        "    # Bottleneck\n",
        "    bn = layers.Conv2D(2048, (3, 3), activation='relu', padding='same')(p5)\n",
        "    bn = layers.Conv2D(2048, (3, 3), activation='relu', padding='same')(bn)\n",
        "\n",
        "    # Decoder\n",
        "    u5 = layers.Conv2DTranspose(1024, (3, 3), strides=(2, 2), padding='same')(bn)\n",
        "    u5 = layers.concatenate([u5, c5])\n",
        "    u5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(u5)\n",
        "    u5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(u5)\n",
        "\n",
        "    u4 = layers.Conv2DTranspose(512, (3, 3), strides=(2, 2), padding='same')(u5)\n",
        "    u4 = layers.concatenate([u4, c4])\n",
        "    u4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(u4)\n",
        "    u4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(u4)\n",
        "\n",
        "    u3 = layers.Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same')(u4)\n",
        "    u3 = layers.concatenate([u3, c3])\n",
        "    u3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u3)\n",
        "    u3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u3)\n",
        "\n",
        "    u2 = layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same')(u3)\n",
        "    u2 = layers.concatenate([u2, c2])\n",
        "    u2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u2)\n",
        "    u2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u2)\n",
        "\n",
        "    u1 = layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same')(u2)\n",
        "    u1 = layers.concatenate([u1, c1])\n",
        "    u1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u1)\n",
        "    u1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u1)\n",
        "\n",
        "    # Output layers for each label\n",
        "    u1_flattened = layers.Flatten()(u1)\n",
        "    output1 = layers.Dense(5, activation='softmax', name='output1')(u1_flattened)\n",
        "    output2 = layers.Dense(5, activation='softmax', name='output2')(u1_flattened)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=[output1, output2])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"model = unet_model()\"\"\"\n",
        "model = build_deep_unet((32, 32, 3))\n",
        "model.compile(optimizer='adam',\n",
        "              loss={'output1': 'categorical_crossentropy', 'output2': 'categorical_crossentropy'},\n",
        "              metrics={'output1': 'accuracy', 'output2': 'accuracy'})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1742s\u001b[0m 3s/step - loss: 2.9439 - output1_accuracy: 0.3346 - output1_loss: 1.4933 - output2_accuracy: 0.3604 - output2_loss: 1.4506\n",
            "Epoch 2/10\n",
            "\u001b[1m319/500\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m10:31\u001b[0m 3s/step - loss: 2.5133 - output1_accuracy: 0.4518 - output1_loss: 1.3047 - output2_accuracy: 0.5135 - output2_loss: 1.2086"
          ]
        },
        {
          "ename": "AbortedError",
          "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall/gradient_tape/functional_2_1/conv2d_28_1/convolution/Conv2DBackpropFilter defined at (most recent call last):\n<stack traces unavailable>\nOperation received an exception:Status: 1, message: could not create a primitive, in file tensorflow/core/kernels/mkl/mkl_conv_grad_filter_ops.cc:685\n\t [[{{node StatefulPartitionedCall/gradient_tape/functional_2_1/conv2d_28_1/convolution/Conv2DBackpropFilter}}]] [Op:__inference_multi_step_on_iterator_246533]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAbortedError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m train_gen \u001b[38;5;241m=\u001b[39m datagenerator(cifar10_x_train_1, cifar10_x_train_2, cifar10_y_train_1, cifar10_y_train_2, \u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mAbortedError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall/gradient_tape/functional_2_1/conv2d_28_1/convolution/Conv2DBackpropFilter defined at (most recent call last):\n<stack traces unavailable>\nOperation received an exception:Status: 1, message: could not create a primitive, in file tensorflow/core/kernels/mkl/mkl_conv_grad_filter_ops.cc:685\n\t [[{{node StatefulPartitionedCall/gradient_tape/functional_2_1/conv2d_28_1/convolution/Conv2DBackpropFilter}}]] [Op:__inference_multi_step_on_iterator_246533]"
          ]
        }
      ],
      "source": [
        "train_gen = datagenerator(cifar10_x_train_1, cifar10_x_train_2, cifar10_y_train_1, cifar10_y_train_2, 64)\n",
        "val_gen = datagenerator(cifar10_x_test_1, cifar10_x_test_2, cifar10_y_test_1, cifar10_y_test_2, 64)\n",
        "\n",
        "history = model.fit(train_gen, steps_per_epoch=500, epochs=10, validation_data=val_gen, validation_steps=100)\n",
        "\n",
        "#model.fit(train_gen, steps_per_epoch=500, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_model(model):\n",
        "    testgen = datagenerator(cifar10_x_test_1, cifar10_x_test_2, cifar10_y_test_1, cifar10_y_test_2, 10000)\n",
        "    eval_samples_x, eval_samples_y = next(testgen)\n",
        "    predictions = model.predict(eval_samples_x)\n",
        "    pred1 = np.argmax(predictions[0], axis=1)\n",
        "    pred2 = np.argmax(predictions[1], axis=1)\n",
        "    correct_guesses_1 = pred1 == np.argmax(eval_samples_y[0], axis=1)\n",
        "    correct_guesses_2 = pred2 == np.argmax(eval_samples_y[1], axis=1)\n",
        "    return (np.mean(correct_guesses_1) + np.mean(correct_guesses_2)) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step\n",
            "Mean accuracy = 0.58802\n",
            "Standard deviation = 0.0028151554131166327\n"
          ]
        }
      ],
      "source": [
        "repeat_eval = 10\n",
        "eval_results = []\n",
        "for i in range(repeat_eval):\n",
        "    eval_results.append(eval_model(model))\n",
        "\n",
        "print(\"Mean accuracy =\", np.mean(eval_results))\n",
        "print(\"Standard deviation =\", np.std(eval_results))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1yTRAzn4i9g"
      },
      "source": [
        "# What to Submit\n",
        "\n",
        "As usual, you need to submit a single notebook that must be executable on Colab. The notebook should be properly commented and include a complete record of the training process, as well as the calculation of accuracy according to the guidelines provided above.\n",
        "\n",
        "# Good luck!\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
